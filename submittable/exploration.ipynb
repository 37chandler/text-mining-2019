{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submittable Exploration\n",
    "\n",
    "In _Telling Stories with Data_ we did a project for Submittable. The organizations that make up Submittable's client base use forms to receive submissions for many different purposes: works for literary publication submissions, contest entries, grant applications, award nominations, etc. Since Submittable's clients don't label their forms, we had to do it by hand. \n",
    "\n",
    "In these two notebooks, we'll do a little work with the form descriptions to see if there are patterns that might be used for labeling. This notebook holds the initial exploration; the second notebook will contain a classifier to attempt to automate the label creation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(nltk.corpus.stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AdminID', 'OrgName', 'OrgDomain', 'OrgUsecases', 'FormID', 'LiveForm', 'FormName', 'FormDescription', 'UseCase', 'Student\\n']\n"
     ]
    }
   ],
   "source": [
    "with open(\"20191112_merged_labeled.txt\",encoding=\"UTF-8\") as infile :\n",
    "    print(infile.readline().split(\"\\t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set has the following columns: \n",
    "\n",
    "* AdminID \n",
    "* OrgName \n",
    "* OrgDomain \n",
    "* OrgUsecases \n",
    "* FormID \n",
    "* LiveForm \n",
    "* FormName \n",
    "* FormDescription \n",
    "* UseCase \n",
    "* Student\n",
    "\n",
    "We're really interested in Form Description and UseCase, so we'll start by just working with those. \n",
    "\n",
    "Many of the forms have multiple use cases because they were labeled by two students. In the data these forms have use cases like `publishing|contest`. For our purposes, we'll consider both labels \"correct\" for now. If the use case has a pipe (`|`) character, we'll split the use cases and store the description twice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We had 444 cases with two different use cases.\n"
     ]
    }
   ],
   "source": [
    "form_data = defaultdict(list)\n",
    "num_doubled = 0\n",
    "\n",
    "with open(\"20191112_merged_labeled.txt\",encoding=\"UTF-8\") as infile :\n",
    "    next(infile)\n",
    "    for row in infile : \n",
    "        row = row.strip().split(\"\\t\")\n",
    "        \n",
    "        use_case = row[8]\n",
    "        description = row[7]\n",
    "        \n",
    "        if \"|\" in use_case :\n",
    "            use_case = use_case.split(\"|\")\n",
    "            num_doubled += 1\n",
    "        else :\n",
    "            use_case = [use_case]\n",
    "            \n",
    "        for uc in use_case :\n",
    "            # For now each use case will just be a list\n",
    "            # of descriptions. \n",
    "            form_data[uc].append(description)\n",
    "    \n",
    "print(f'We had {num_doubled} cases with two different use cases.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps we can start by looking at how many descriptions we have, how many total words, how many unique words, and how many words that aren't stopwords. \n",
    "\n",
    "It'll be useful to have a function that turns a description into a clean, \"bag of words\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag(desc) : \n",
    "    \"\"\"Turns a description into a bag of clean words.\"\"\"\n",
    "    words = [w.lower() for w in desc.split()]\n",
    "    words = [w for w in words if w.isalpha() and w not in sw]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 605730 total words.\n",
      "We have 22108 unique words.\n",
      "We have 22108 unique non-stopwords.\n"
     ]
    }
   ],
   "source": [
    "desc_words = []\n",
    "\n",
    "for uc in form_data :\n",
    "    for desc in form_data[uc] :\n",
    "        desc_words.extend(bag(desc))\n",
    "        \n",
    "        \n",
    "print(f\"We have {len(desc_words)} total words.\")\n",
    "print(f\"We have {len(set(desc_words))} unique words.\")\n",
    "print(f\"We have {len(set(desc_words) - sw)} unique non-stopwords.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks promising. 1.1M words, 22K are unique and most of those aren't stopwords. What are the most common ones? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('must', 5899),\n",
       " ('work', 5774),\n",
       " ('please', 5313),\n",
       " ('may', 4592),\n",
       " ('link', 4155),\n",
       " ('application', 3999),\n",
       " ('submit', 3940),\n",
       " ('one', 3322),\n",
       " ('arts', 2441),\n",
       " ('artists', 2393),\n",
       " ('submissions', 2381),\n",
       " ('program', 2330),\n",
       " ('include', 2311),\n",
       " ('new', 2292),\n",
       " ('submission', 2255),\n",
       " ('us', 2094),\n",
       " ('information', 2062),\n",
       " ('art', 2059),\n",
       " ('artist', 2048),\n",
       " ('project', 1995)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([w for w in desc_words if w not in sw]).most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's just count descriptions by use case, then calculate those summary statistics by use-case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_case                num_forms  num_words   unique-non-sw   words_per_form\n",
      "publishing                   1433     108880            8807             6.15\n",
      "grants                        664      79398            7577            11.41\n",
      "contest                       590      63833            6648            11.27\n",
      "job applications              414      68469            7527            18.18\n",
      "award/nomination              353      38478            5105            14.46\n",
      "exhibition                    324      54317            6540            20.19\n",
      "fellowships                   302      40979            5486            18.17\n",
      "admissions                    265      24774            4176            15.76\n",
      "festival or event             193      30061            4977            25.79\n",
      "conference                    176      18573            4008            22.77\n",
      "internal use                  170      10424            2888            16.99\n",
      "residency                     143      24648            3675             25.7\n",
      "peer review                   141      11887            3150            22.34\n",
      "scholarships                  119      12215            2666             22.4\n",
      "audition                       99      10605            2775            28.03\n",
      "NA                             51       5073            2027            39.75\n",
      "other                          42       3116            1407             33.5\n"
     ]
    }
   ],
   "source": [
    "print(\"use_case                num_forms  num_words   unique-non-sw   words_per_form\")\n",
    "\n",
    "# Note the tricky lambda function to sort by popularity descending\n",
    "for uc in sorted(form_data,key=lambda u: -1*len(form_data[u]) )  :\n",
    "    these_descs = form_data[uc]\n",
    "    num_forms = len(these_descs)\n",
    "    \n",
    "    these_words = []\n",
    "    for desc in these_descs :\n",
    "        these_words.extend(bag(desc))\n",
    "    \n",
    "    num_words = len(these_words)\n",
    "    num_uni_non_sw = len(set(these_words)-sw)\n",
    "    \n",
    "                \n",
    "    print(f\"{uc:20} {num_forms:12} {num_words:10} {num_uni_non_sw:15} {round(num_uni_non_sw/num_forms,2):16}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations from these descriptive statistics: \n",
    "\n",
    "* Publishing makes up about 25% of total forms. Grants and contests are 12% and 10% respectively. There are 17 use cases, with most being a small percentage of the total. This is hard on classification algorithms, since guessing \"publishing\" will be right 1/4 of the time. \n",
    "* Most of the unique words we have aren't stopwords. Not surprising, since the stopword set isn't huge. \n",
    "* The total number of words starts getting small as we move past spot 10 in the list. It'd be surprising if we had enough information for, say, \"internal use\". \n",
    "* The words per form are variable. Publishing has a shocking small number of words. That's worthy of further scrutiny. Generally it seems like use cases with fewer forms use more unique words. That makes sense, probably, as submitters may need more information. \n",
    "\n",
    "Some of these use cases have low average number of unique words. Let's take a look at the distribution for publishing, grants, and contests. There's probably a histogram function in numpy somewhere, but I don't know it, so I'm going write my own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distro(uc,fd) :\n",
    "    \"\"\" Given a use case (uc) and our form data (fd),\n",
    "        calculate the distribution of description lengths. \n",
    "        Returns a dictionary with bin labels and counts.\n",
    "        \"\"\"\n",
    "    bin_cutoff = [10,25,50,75,125,200]\n",
    "    results = defaultdict(int)\n",
    "    \n",
    "    for desc in fd[uc] :\n",
    "        desc_words = bag(desc) \n",
    "        num = len(set(desc_words)) \n",
    "        \n",
    "        lb = 0 \n",
    "        \n",
    "        for ub in bin_cutoff :\n",
    "            if lb < num <= ub :\n",
    "                break\n",
    "            else :\n",
    "                lb = ub \n",
    "                \n",
    "        if lb != ub :\n",
    "            label = str(lb) + \"-\" + str(ub-1) \n",
    "        else :\n",
    "            label = str(ub) + \"+\"\n",
    "            \n",
    "        results[label] += 1\n",
    "        \n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that out of the way, let's look at the distribution of unique words for our three biggest use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Distribution for Publishers\n",
      "\n",
      "Num Words in Desc     Descriptions\n",
      "0-9                            178\n",
      "10-24                          270\n",
      "25-49                          376\n",
      "50-74                          219\n",
      "75-124                         218\n",
      "125-199                        122\n",
      "200+                            50\n"
     ]
    }
   ],
   "source": [
    "ranges = ['0-9','10-24','25-49','50-74','75-124','125-199','200+']\n",
    "pub_distro = get_distro(\"publishing\",form_data)\n",
    "\n",
    "# Test that I didn't change the bins\n",
    "assert(set(ranges)-set(pub_distro.keys())==set())\n",
    "\n",
    "print(\"Word Distribution for Publishers\")\n",
    "print('')\n",
    "\n",
    "print('Num Words in Desc     Descriptions')\n",
    "for rng in ranges :\n",
    "    print(f'{rng:20} {pub_distro[rng]:13}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These bins were chosen for this category, so the distribution is pretty even. We see 178 very short descriptions (under 10 unique non-stopwords). There are about a similar number with descriptions longer than 125 words. Overall things don't look *too* concerning, although I'd like to see some of these very short descriptions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Distribution for Grants\n",
      "\n",
      "Num Words in Desc     Descriptions\n",
      "0-9                             36\n",
      "10-24                           99\n",
      "25-49                          152\n",
      "50-74                          110\n",
      "75-124                         132\n",
      "125-199                         84\n",
      "200+                            51\n"
     ]
    }
   ],
   "source": [
    "pub_distro = get_distro(\"grants\",form_data)\n",
    "\n",
    "print(\"Word Distribution for Grants\")\n",
    "print('')\n",
    "\n",
    "print('Num Words in Desc     Descriptions')\n",
    "for rng in ranges :\n",
    "    print(f'{rng:20} {pub_distro[rng]:13}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grants appear to have a distribution closer to normal, with most falling in the 25-125 word range. Still some long ones out there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Distribution for Contests\n",
      "\n",
      "Num Words in Desc     Descriptions\n",
      "0-9                             51\n",
      "10-24                           71\n",
      "25-49                          135\n",
      "50-74                          122\n",
      "75-124                         113\n",
      "125-199                         63\n",
      "200+                            35\n"
     ]
    }
   ],
   "source": [
    "pub_distro = get_distro(\"contest\",form_data)\n",
    "\n",
    "print(\"Word Distribution for Contests\")\n",
    "print('')\n",
    "\n",
    "print('Num Words in Desc     Descriptions')\n",
    "for rng in ranges :\n",
    "    print(f'{rng:20} {pub_distro[rng]:13}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contests look pretty similar to grants, with a slight skew toward shorter forms. \n",
    "\n",
    "Let's look at some of the very short form descriptions for publishers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>Submit one short screenplay or one-act play of no more than 10 pages. Please do not include identifying information on the submission. </p><br>\n",
      "\n",
      "\"<div class=\"\"clearfix\"\"><p>1 piece<br> Upload in high resolution or 300 dpi for scanning photography.</p></div>\"\n",
      "\n",
      "\"<p>Submit one piece of creative nonfiction of up to 4,000 words.</p><p>Please include a third-person bio of fewer than 75 words.</p><br>\"\n",
      "\n",
      "\"<div class=\"\"clearfix\"\"><p>Poems in traditional and experimental styles but no light verse (up to 6 poems).</p><p>Please include the following contact information in your cover letter and/or on your manuscript: mailing address, phone number, and email address if available.</p></div>\"\n",
      "\n",
      "\"<p>We’re interested in <b>black-and-white</b> photographs. We’re not looking for photojournalism, just unique perspectives on the world around us — especially human interactions.</p><p>Please review our full <a target=\"\"_blank\"\" rel=\"\"nofollow\"\" href=\"\"https://www.thesunmagazine.org/submit#photography\"\">submission guidelines and sample photographs</a> before sending us your work.</p>\"\n",
      "\n",
      "\"<div class=\"\"clearfix\"\"><p>Every week, Typishly sends out a call for Short Fiction Mondays, with a Monday submission deadline. We will read and respond to your submission in less than one week.<br>- One story per submission. You're welcome to make more than one submission if you want us to read more than one of your stories.<br>- Don’t submit short fiction over 5,000 words long.</p></div>\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_printed = 0\n",
    "\n",
    "for desc in form_data['publishing'] :\n",
    "    uni_words = set(bag(desc))\n",
    "    \n",
    "    if len(uni_words) < 20 :\n",
    "        print(desc)\n",
    "        print()\n",
    "        num_printed += 1\n",
    "        \n",
    "        if num_printed > 5 :\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, I'm not sure what I learned from that. Some people use really, really short descriptions. \n",
    "\n",
    "One thing I _did_ notice is that some forms, like the first publishing one, have some typos brought on in the cleaning process. Look for text that contains `poemsthat`. Could definitely use the spell checking stuff to correct these, though I'm not going to worry about it now. Putting in a comment for posterity. \n",
    "\n",
    "---\n",
    "\n",
    "### Frequency Distributions\n",
    "\n",
    "Let's look at a few frequency distributions by use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- publishing -----------\n",
      "[('please', 1604),\n",
      " ('work', 1529),\n",
      " ('submissions', 1302),\n",
      " ('submit', 1267),\n",
      " ('us', 1125),\n",
      " ('one', 1081),\n",
      " ('submission', 997),\n",
      " ('may', 817),\n",
      " ('must', 773),\n",
      " ('include', 742),\n",
      " ('poems', 589),\n",
      " ('link', 585),\n",
      " ('published', 573),\n",
      " ('poetry', 551),\n",
      " ('short', 539)]\n",
      "\n",
      "---------- grants -----------\n",
      "[('grant', 1168),\n",
      " ('must', 970),\n",
      " ('application', 949),\n",
      " ('link', 797),\n",
      " ('project', 690),\n",
      " ('arts', 643),\n",
      " ('may', 610),\n",
      " ('please', 588),\n",
      " ('support', 530),\n",
      " ('program', 438),\n",
      " ('submit', 437),\n",
      " ('community', 408),\n",
      " ('work', 395),\n",
      " ('grants', 388),\n",
      " ('funding', 354)]\n",
      "\n",
      "---------- contest -----------\n",
      "[('must', 950),\n",
      " ('may', 714),\n",
      " ('submit', 555),\n",
      " ('link', 550),\n",
      " ('please', 512),\n",
      " ('entry', 494),\n",
      " ('work', 450),\n",
      " ('one', 374),\n",
      " ('entries', 356),\n",
      " ('submission', 347),\n",
      " ('submissions', 328),\n",
      " ('submitted', 302),\n",
      " ('use', 274),\n",
      " ('include', 267),\n",
      " ('artist', 262)]\n",
      "\n",
      "---------- job applications -----------\n",
      "[('work', 720),\n",
      " ('please', 426),\n",
      " ('experience', 335),\n",
      " ('application', 318),\n",
      " ('quality', 288),\n",
      " ('program', 283),\n",
      " ('ability', 271),\n",
      " ('including', 264),\n",
      " ('support', 264),\n",
      " ('may', 262),\n",
      " ('team', 259),\n",
      " ('information', 259),\n",
      " ('de', 245),\n",
      " ('care', 233),\n",
      " ('writing', 228)]\n",
      "\n",
      "---------- award/nomination -----------\n",
      "[('must', 584),\n",
      " ('award', 392),\n",
      " ('may', 353),\n",
      " ('work', 335),\n",
      " ('please', 298),\n",
      " ('awards', 255),\n",
      " ('submit', 255),\n",
      " ('entry', 248),\n",
      " ('link', 244),\n",
      " ('one', 219),\n",
      " ('arts', 175),\n",
      " ('include', 170),\n",
      " ('submitted', 170),\n",
      " ('application', 136),\n",
      " ('entries', 121)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for uc in sorted(form_data,key=lambda u: -1*len(form_data[u]) ) :\n",
    "    these_descs = form_data[uc]\n",
    "    these_words = [] # should I have just stored these? Maybe\n",
    "    for desc in these_descs :\n",
    "        these_words.extend(bag(desc))\n",
    "        \n",
    "    fd = nltk.FreqDist(these_words)\n",
    "    \n",
    "    print(\"---------- \" + uc + \" -----------\")\n",
    "    pprint(fd.most_common(15))\n",
    "    print()\n",
    "\n",
    "    if \"award\" in uc : \n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this seems pretty promising. \n",
    "\n",
    "One last piece of exploration. For each use case, I'd like to go through and find five words that are in the top 50 for that use case that aren't in the top 50 for any other. I'll try brute force first, going through each use case and getting the top $N$ words for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top = 50\n",
    "top_by_uc = dict()\n",
    "\n",
    "for uc in sorted(form_data,key=lambda u: -1*len(form_data[u]) ) :\n",
    "    these_descs = form_data[uc]\n",
    "    these_words = [] # should I have just stored these? Maybe\n",
    "    for desc in these_descs :\n",
    "        these_words.extend(bag(desc))\n",
    "        \n",
    "    fd = nltk.FreqDist(these_words)\n",
    "\n",
    "    top_by_uc[uc] = set([w for w,cnt in fd.most_common(num_top)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a second pass, finding the ones that are unique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publishing: read,previously,publish,accept,send,poetry,submitting,words,story\n",
      "\n",
      "grants: cultural,fund,funds,organizations,grants,funding,report,final\n",
      "\n",
      "contest: photos,competition,prize,contest\n",
      "\n",
      "job applications: care,skills,quality,internship,ability,aged,national,management,position,communication\n",
      "\n",
      "award/nomination: individual,nomination,winner,awards,nominations,best,annual\n",
      "\n",
      "exhibition: additional,exhibition,works,scad,gallery,members,artwork\n",
      "\n",
      "fellowships: completed,social,based,applying,fellows\n",
      "\n",
      "admissions: focus,interested,course,pm\n",
      "\n",
      "festival or event: agreement,light,shall,film,vendors,festival,insurance,vendor,bopa,market\n",
      "\n",
      "conference: session,abstract,workshops,place,proposal,author,conference,poster,sessions,panel,presentations\n",
      "\n",
      "internal use: people,ensure,payroll,prior,staff\n",
      "\n",
      "residency: residency,file,statement,sample,senior,studio,want\n",
      "\n",
      "peer review: pages,looking\n",
      "\n",
      "scholarships: recipients,committee,least,financial,current,academic,ñ,scholarship,high,scholarships,awarded,college\n",
      "\n",
      "audition: june,perform,performance,audition,deadline,july,music,state\n",
      "\n",
      "NA: environmental,downtown,street,synthesis,main,ideas,idea\n",
      "\n",
      "other: et,votre,la,full,theater,teach,due,le,budget,play,roster\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for uc in sorted(form_data,key=lambda u: -1*len(form_data[u]) ) :\n",
    "    these_descs = form_data[uc]\n",
    "    these_words = [] # should I have just stored these? Maybe\n",
    "    for desc in these_descs :\n",
    "        these_words.extend(bag(desc))\n",
    "        \n",
    "    fd = nltk.FreqDist(these_words)\n",
    "    top_n = {w for w, cnt in fd.most_common(num_top)}\n",
    "\n",
    "    other_tops = set()\n",
    "    for other_uc in top_by_uc :\n",
    "        if other_uc != uc :\n",
    "            other_tops = other_tops.union(top_by_uc[other_uc])\n",
    "    \n",
    "    \n",
    "    unique = top_n - other_tops\n",
    "\n",
    "    if unique :\n",
    "        print(uc + \": \" + \",\".join(unique))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this seems like it has real potential. Not sure if Naive Bayes will be able to categorize properly, but it seems possible. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
