{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Classification with Naive Bayes\n",
    "\n",
    "In this notebook, we'll try to predict which twitter user follows which handle you selected. The goals here are potentially two-fold. \n",
    "\n",
    "1. Build a Classifier: We might be legitimately interested in classification. For instance, we could do this along some dimension we might care about. Then we could take any description and score it along this dimension. Could you do this with just general text? What might be the strengths and weaknesses of doing so?\n",
    "1. Naive Bayes (NB) for Exploration: If we just want to understand how two groups use (this very particular sub-species of) language, NB could help us do it. As we'll see below, the `show_most_informative_features` for sets of words can give us a view into the raw language that's being used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "from string import punctuation\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by simply using the words in descriptions. First, let's read in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you'll need to replace your file names and labels here. \n",
    "\n",
    "d = []\n",
    "\n",
    "with open(\"20191014_GeneralMills_followers.txt\",'r') as infile :\n",
    "    next(infile)\n",
    "    \n",
    "    for line in infile.readlines() :\n",
    "        line = line.strip(\"\\n\").split(\"\\t\") # need to specify what we're stripping here.\n",
    "        \n",
    "        if line[6] : # test for empty description\n",
    "            d.append((line[6],'big_food'))\n",
    "\n",
    "with open(\"20191014_michaelpollan_followers.txt\",'r') as infile :\n",
    "    next(infile)\n",
    "    \n",
    "    for line in infile.readlines() :\n",
    "        line = line.strip(\"\\n\").split(\"\\t\")\n",
    "        if line[6] :\n",
    "            d.append((line[6],'pollan'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, let's look at a little of the data. I'll shuffle it first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(d)\n",
    "sample = d[:5]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write a function that cleans up the description and maps it on to words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def desc_features(the_description) :\n",
    "    \"\"\" Input: A twitter description\n",
    "        Output: A dictionary listin the words that are in \n",
    "                the description.\n",
    "                \n",
    "        This function does some cleaning on the descriptions,\n",
    "        removing some punctuation, splitting on whitespace, \n",
    "        dropping to lower case. It returns a dictionary \n",
    "        of the form \n",
    "            {example : True,\n",
    "             word :    True}\n",
    "    \n",
    "        \"\"\"\n",
    "    exclude = set(punctuation)\n",
    "    exclude.remove(\"#\") #useful for twitter...\n",
    "    \n",
    "    # Found this at https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string-in-python\n",
    "    the_description = ''.join([ch.lower() for ch in the_description if ch not in exclude])\n",
    "    \n",
    "    word_list = the_description.split()\n",
    "\n",
    "    ret_val = {}\n",
    "    \n",
    "    for word in word_list :\n",
    "        ret_val[word] = True\n",
    "    \n",
    "    return(ret_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, it's a good idea to test your functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in sample :\n",
    "    desc, label = a\n",
    "    print(\"Started with: \" + desc)\n",
    "    print(\"-------------------Then got---------------------------\")\n",
    "    pprint(desc_features(desc))\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we're ready to do the NB stuff. It's actually shockingly easy at this point, since we've done the work to set it up. We've got 255K total descriptions (found by typing `len(d)` in a cell). That's big enough that I'll use a full 5000 for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 5000\n",
    "\n",
    "featuresets = [(desc_features(desc), label) for (desc, label) in d]\n",
    "train_set, test_set = featuresets[test_set_size:], featuresets[:test_set_size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How'd we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not terrible, assuming it's about a 50/50 split. Let's see what that is, using my trick from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter([label for desc, label in d])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, in my example Pollan is about 83% of the data, so we're not doing better than if we just guessed \"pollan\" all the time. Well, we can think about making it better in a minute. For now, let's see what's predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of the big food features seem to be pretty \"spammy\". For instance \"freebie\", \"#sweepstakes\", \"#giveaways\". Although some of them seem legit like those related to coupons or pillsbury. \n",
    "\n",
    "If I was going to try to improve it, here's some stuff I'd try:\n",
    "\n",
    "1. Remove stopwords: Remember, these are those common words that don't carry a lot of meaning. Might not matter, but it'd be cleaner and faster. \n",
    "1. Limit the model to just the top $N$ remaining words. Not sure what to pick for $N$, but I'd try 1000 or so. It'd be worth it to do the whole `train_set/dev_test_set/test_set` if we were headed down this path and we could try a bunch of $N$s. \n",
    "1. See if number of followers is predictive. Using continuous variables in Naive Bayes is [a bit tricky](https://stats.stackexchange.com/questions/61034/naive-bayes-on-continuous-variables), but it can sometimes be quite helpful. \n",
    "\n",
    "From an exploratory standpoint, I might be able to get more interesting results by sampling my more pervasive class (pollan for me). Let's take a look at that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get 60K from Pollan\n",
    "new_d = [item for item in d if item[1]==\"big_food\"]\n",
    "pollan = [item for item in d if item[1]==\"pollan\"]\n",
    " \n",
    "new_d.extend(random.choices(pollan,k=65000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did we get what we expected? \n",
    "Counter([label for desc, label in new_d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_size = 5000\n",
    "\n",
    "featuresets = [(desc_features(desc), label) for (desc, label) in new_d]\n",
    "train_set, test_set = featuresets[test_set_size:], featuresets[:test_set_size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting more words that seem interesting both ways. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm curious to view these words \"in situ\"\n",
    "\n",
    "count = 0\n",
    "for item in d :\n",
    "    desc, label = item\n",
    "    \n",
    "    if (\"herbalist\" in desc) :\n",
    "        print(desc)\n",
    "        print(label)\n",
    "        print(\"\\n\")\n",
    "        count += 1\n",
    "\n",
    "    if count > 5 :\n",
    "        break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these features seem more informative? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
