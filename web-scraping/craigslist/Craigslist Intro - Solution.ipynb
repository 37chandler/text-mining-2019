{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tees us up to do some interesting scraping of Craigslist sites. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import time # used to make our code \"sleep\" for a second or two before new calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by doing a car search on Craigslist and looking at the results. Click on this link and feel free to change the search terms once you get there: https://seattle.craigslist.org/search/cto?query=volkswagen+jetta. \n",
    "\n",
    "Once you've looked around a bit (including looking at the page source and page elements), enter the URL down below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_search_page = \"https://seattle.craigslist.org/search/cto?query=volkswagen+jetta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the page and process it with Beautiful Soup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(my_search_page)\n",
    "print(r.status_code)\n",
    "\n",
    "bs = BeautifulSoup(r.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.prettify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, not that pretty. Okay, we've come to our first challenge. As you can see from the page when you open it in a browser, there are a bunch of cars listed on that page. If you right click on one you'll be able to copy the car listing. It'll look kind of like this `https://seattle.craigslist.org/skc/cto/d/seattle-jetta-2005/6994660904.html`. \n",
    "\n",
    "### First challenge\n",
    "\n",
    "Extract every one of those links from the page you're on. Store them in in a list call `car_pages`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach: Let's try to identify those page elements via the \"inspect element\" on Chrome. It looks like they show up in blocks that look like this: \n",
    "\n",
    "```\n",
    "<a \n",
    "    href=\"https://seattle.craigslist.org/tac/cto/d/tacoma-2001-volkswagen-jetta/6994004871.html\" \n",
    "    class=\"result-image gallery\" \n",
    "    data-ids=\"1:00B0B_kGzRKlMhcLE,1:00303_ivZbPUU6nb1,1:00n0n_32JBe7nYXP,\n",
    "              1:00W0W_1AOugnLjUvc,1:00c0c_kB6girnJdhA,1:00c0c_gJQhKdxhS5o,\n",
    "              1:00L0L_eyvNJpLqaPN,1:00x0x_4ZUhwK5KmER\">\n",
    "    <span class=\"result-price\">$1700</span>\n",
    "</a>\n",
    "```\n",
    "\n",
    "So we'll want to look for `a` tags that have class `result-image gallery`. (There are probably lots of ways to do this, but going for this pair seems like the easiest to me. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's just print the class of the links on the page.\n",
    "for link in bs.find_all('a'):\n",
    "    print(link.get('class'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, it looks like the class for the ones we want is `['result-image', 'gallery']`, so let's test for that and grab the links. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_pages = []\n",
    "\n",
    "for link in bs.find_all('a') :\n",
    "    if (link.get('class') and # some links don't have classes, so need to test for its existence  \n",
    "        ('result-image' in link.get('class') and\n",
    "        'gallery' in link.get('class'))) :\n",
    "        \n",
    "        car_pages.append(link.get('href'))\n",
    "        \n",
    "print(len(car_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, in the version I just ran we have 120 links. Looking at them, they look pretty reasonable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_car_pages = car_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Challenge\n",
    "\n",
    "Okay, you may want to do these out of order, so let's define a `car_pages` variable so you have something to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_pages = ['https://seattle.craigslist.org/skc/cto/d/seattle-jetta-2005/6994660904.html',\n",
    "             'https://seattle.craigslist.org/skc/cto/d/federal-way-2003-vw-jetta/6994207516.html',\n",
    "             'https://seattle.craigslist.org/see/cto/d/federal-way-2014-volkswagen-jetta-20l/6994139994.html']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now we're ready for the second challenge. Extract some of the key elements from this the resulting Craigslist page. We'll discuss this in class, but some good ones to extract: \n",
    "\n",
    "* Make\n",
    "* Model\n",
    "* Price\n",
    "* Transmission\n",
    "* Miles\n",
    "* Number of images\n",
    "* Description\n",
    "* Page title\n",
    "\n",
    "Craigslist isn't trying to make things easy on us. But there is a box on the right-hand side where a bunch of these fields are being stored. Let's just grab those. Once you've done that, your goal is to scrape all the pages in `car_pages` and write out the info to a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = car_pages[0]\n",
    "\n",
    "r = requests.get(page)\n",
    "bs = BeautifulSoup(r.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going to write a function that covers most of the obvious cases. This \n",
    "# is clearly overkill. I'll return a dictionary with all the values, including\n",
    "# the missing ones. \n",
    "def get_vehicle_info(soup) :\n",
    "    ''' Given soup, returns a dictionary with \n",
    "        most of the vehicle info. vin, cond, cyl, drive,\n",
    "        fuel, odometer, paint_color, size, trans, type.\n",
    "    '''\n",
    "    \n",
    "    car_text = soup.find_all(\"p\", {\"class\":\"attrgroup\"})[1]\n",
    "    \n",
    "    # Set all values to None to start\n",
    "    vin = cond = cyl = drive = fuel = None\n",
    "    odometer = title_status = trans = None\n",
    "    color = size = type_val = None\n",
    "\n",
    "    for item in car_text.find_all(\"span\") :\n",
    "        if \"VIN\" in str(item) :\n",
    "            vin = item.b.string\n",
    "        elif \"condition\" in str(item) :\n",
    "            cond = item.b.string\n",
    "        elif \"cylinders\" in str(item) :\n",
    "            cyl = item.b.string \n",
    "        elif \"drive\" in str(item) :\n",
    "            drive = item.b.string \n",
    "        elif \"fuel\" in str(item) :\n",
    "            fuel = item.b.string \n",
    "        elif \"odometer\" in str(item) :\n",
    "            odometer = item.b.string \n",
    "        elif \"title status\" in str(item) :\n",
    "            title_status = item.b.string \n",
    "        elif \"transmission\" in str(item) :\n",
    "            trans = item.b.string \n",
    "        elif \"paint color\" in str(item) :\n",
    "            color = item.b.string \n",
    "        elif \"size\" in str(item) :\n",
    "            size = item.b.string\n",
    "        elif \"type\" in str(item) :\n",
    "            type_val = item.b.string\n",
    "\n",
    "            \n",
    "            \n",
    "    return_dict = {\"vin\":vin,\n",
    "                   \"condition\":cond,\n",
    "                   \"cylinders\":cyl,\n",
    "                   \"drive\":drive,\n",
    "                   \"fuel\":fuel,\n",
    "                   \"odometer\":odometer,\n",
    "                   \"title_status\":title_status,\n",
    "                   \"transmission\":trans,\n",
    "                   \"paint_color\":color,\n",
    "                   \"size\":size,\n",
    "                   \"type\":type_val}\n",
    "  \n",
    "    return(return_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's test it\n",
    "get_vehicle_info(bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that looks pretty good. Now let's write a cell that extract all the info and writes each one to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"20191007_car_scrape.txt\"\n",
    "written_headers = False # notice the use of this trick. It's a good one. \n",
    "\n",
    "with open(output_file,'w') as ofile :\n",
    "    for page in car_pages :\n",
    "        r = requests.get(page)\n",
    "        if r.status_code == 200 :\n",
    "            bs = BeautifulSoup(r.text,'html.parser')\n",
    "\n",
    "            vehicle_info = get_vehicle_info(bs)\n",
    "\n",
    "            if not written_headers :\n",
    "                headers = ['page']\n",
    "\n",
    "                # build up our headers list\n",
    "                for key in vehicle_info :\n",
    "                    headers.append(key)\n",
    "                    \n",
    "                ofile.write(\"\\t\".join(headers) + \"\\n\")\n",
    "                written_headers = True\n",
    "\n",
    "            # Now we build up the row. We're going to use\n",
    "            # the existence of the headers list to access\n",
    "            # the keys of the dictionary in the correct order.\n",
    "            # Remember: we can't depend on the ordering of \n",
    "            # a dictionary. \n",
    "\n",
    "            output_row = [page]\n",
    "            \n",
    "            for field_name in headers[1:] : # skip first spot\n",
    "                output_row.append(vehicle_info[field_name])\n",
    "                \n",
    "            ofile.write(\"\\t\".join([str(item) for item in output_row]) + \"\\n\")\n",
    "\n",
    "        time.sleep(1)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, nice work. One big piece that's missing is price. For a bonus challenge, try to get the price off the page. Open question: why did CL make it so hard to get price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This one is a monster. I grabbed it from the full CL project I have, so it gets a _lot_ of other stuff that's useful.\n",
    "def get_page_elements(soup) :\n",
    "    ''' Getting elements of the page that \n",
    "      *should* be mostly straightforward:\n",
    "      post_id, listing_name, posting_body_text,\n",
    "      title_text, posting_dt, update_dt \n",
    "      price\n",
    "    '''\n",
    "  \n",
    "    # post-id\n",
    "    post_id = None\n",
    "    for item in soup.find_all(\"p\",{\"class\":\"postinginfo\"}) :\n",
    "        if \"post\" in str(item.string) :\n",
    "            post_id = item.string.split(\":\")[1]\n",
    "            post_id = post_id.strip()\n",
    "  \n",
    "    # listing_name\n",
    "    # TODO: Not sure what this is. I put it in the \n",
    "    # db design, but don't know what it means. Leaving \n",
    "    # it in for now in the hopes that I'll remember\n",
    "    # what I was seeing.\n",
    "    # TODO: It's been a while. Remove this. \n",
    "  \n",
    "    listing_name = None\n",
    "\n",
    "    # posting_body_text\n",
    "    posting_body_text = soup.find_all(\"section\",{\"id\":\"postingbody\"})[0].text.strip() \n",
    "\n",
    "    # title_text\n",
    "    full_title_text = soup.find_all(\"span\",{\"class\":\"postingtitletext\"})[0].text.split(\"\\n\")\n",
    "    full_title_text = [a for a in full_title_text if len(a) > 0]  \n",
    "\n",
    "    # attempt at actual title. Is hyphen split reliable?\n",
    "    title_text = full_title_text[0].split(\"-\")[0].strip()\n",
    "\n",
    "    # grab price and location\n",
    "    price = location = None\n",
    "    hyphens = full_title_text[0].count(\"-\")\n",
    "  \n",
    "    if hyphens > 0 :    \n",
    "        # This is tricky. It's possible to have\n",
    "        # multiple hyphens in the title text. \n",
    "        if hyphens == 1 :\n",
    "            price_spot = 1\n",
    "        elif hyphens == 2 :\n",
    "            price_spot = 2\n",
    "        else :\n",
    "            price_spot = hyphens # hope for the best\n",
    "\n",
    "    \n",
    "    price_loc_part = full_title_text[0].split(\"-\")[price_spot].strip()\n",
    "    price = price_loc_part.split(\"(\")[0].strip()\n",
    "    price = price.replace(\"$\",\"\")\n",
    "    # TODO: add error checking and logging. Also, \n",
    "    # this could be separated out of the pull itself.\n",
    "    # \n",
    "    # 20181128: Not sure what this note actually means? am I \n",
    "    # saying that I should write an \"extract price\" function?\n",
    "    # that seems like a reasonable idea. \n",
    "  \n",
    "    # location: the place listed in the title\n",
    "    if \"(\" in price_loc_part :\n",
    "        location = price_loc_part.split(\"(\")[1].strip()\n",
    "        location = location.replace(\")\",\"\")\n",
    "  \n",
    "    full_title_text = \"\".join(full_title_text) \n",
    "    # For now we're storing this so we can figure out if we're screwing it up.\n",
    "\n",
    "    # times\n",
    "    posting_dt = update_dt = None\n",
    "  \n",
    "    for item in soup.find_all(\"p\",{\"class\":\"postinginfo reveal\"}) :\n",
    "        if \"posted:\" in item.text :\n",
    "            holder = item.find_all(\"time\")[0]\n",
    "            posting_dt = holder.attrs['datetime']\n",
    "        elif \"updated:\" in item.text :\n",
    "            holder = item.find_all(\"time\")[0]\n",
    "            update_dt = holder.attrs['datetime']  \n",
    "      \n",
    "    # Num images\n",
    "    holder = soup.find_all(\"div\",{\"id\":\"thumbs\"})\n",
    "    if len(holder) > 0 :\n",
    "        num_images = len(holder[0].find_all(\"a\"))    \n",
    "    else :\n",
    "        num_images = 0\n",
    "\n",
    "    ret_d = {\"post_id\":post_id,\n",
    "             \"listing_name\":listing_name,\n",
    "             \"posting_body_text\":posting_body_text,\n",
    "             \"full_title_text\":full_title_text,\n",
    "             \"title_text\":title_text,\n",
    "             \"posting_dt\":posting_dt,\n",
    "             \"update_dt\":update_dt,\n",
    "             \"price\":price,\n",
    "             \"location\":location,\n",
    "             \"num_images\":num_images}\n",
    "  \n",
    "    return(ret_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_page_elements(bs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
