{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Candidates' Sites\n",
    "\n",
    "In this notebook we'll scrape candidates' websites and store the information for later analysis. We'll store the data locally in a database, which we'll build via Python to practice that skill. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4.element import Comment\n",
    "\n",
    "import random \n",
    "# this package is good for randomization, which we'll \n",
    "# use when we're pulling a fraction of the pages. \n",
    "\n",
    "import datetime # this is good for working with dates and times. It's a bit confusing though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build a DB for this. Right now I'm assuming we'll have these fields in our only table:\n",
    "\n",
    "* `dt`: the date and time when we pulled the page.\n",
    "* `base_url`: the main URL we're pulling from. E.g., www.joebiden.com, for Biden's site. \n",
    "* `url`: the specific URL we're pulling from. E.g., https://joebiden.com/joes-story/.\n",
    "* `text`: the text of the `url`. \n",
    "* `pulled`: A boolean that is TRUE if we've tried to pull the text from the URL. This is useful for keeping track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\"candidate_websites.db\") # feel free to change this to something you like. \n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the table in the DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS site_text''')\n",
    "cur.execute('''CREATE TABLE site_text (\n",
    "    dt DATETIME, \n",
    "    base_url TEXT, \n",
    "    url TEXT,\n",
    "    text TEXT,\n",
    "    pulled BOOLEAN)''')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build off the previous notebook (`Intro to Scraping.ipynb`) to scrape candidates' sites. Let's begin by reading the list of websites.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = []\n",
    "with open(\"candidates_websites.txt\",'r') as infile :\n",
    "    for line in infile :\n",
    "        sites.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for candi_site in sites :\n",
    "    \n",
    "    r = requests.get(candi_site)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    all_a_tags = soup.find_all('a')\n",
    "    \n",
    "    links = [candi_site] # make sure the main page is in there. \n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    # Now we have all the links. Let's just load them into the DB. We'll do the \n",
    "    # page pulls and parsing separately. \n",
    "    \n",
    "    for link in set(links) : # wrap a `set` around it so that we don't get duplicates\n",
    "        new_row = [datetime.datetime.now(),\n",
    "                   candi_site,\n",
    "                   link,\n",
    "                   \"\", # empty string for text\n",
    "                   False]\n",
    "        \n",
    "        cur.execute('''INSERT INTO site_text (dt,base_url,url,text,pulled) \n",
    "               VALUES (?,?,?,?,?)''',new_row)\n",
    "        \n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look and see how many pages we have per candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original site https://amyklobuchar.com has 62 links within it.\n",
      "Original site https://peteforamerica.com/ has 54 links within it.\n",
      "Original site https://kamalaharris.org/ has 42 links within it.\n",
      "Original site https://berniesanders.com/ has 40 links within it.\n",
      "Original site https://betoorourke.com/ has 38 links within it.\n",
      "Original site https://elizabethwarren.com has 37 links within it.\n",
      "Original site https://www.donaldjtrump.com/ has 36 links within it.\n",
      "Original site https://www.yang2020.com/ has 34 links within it.\n",
      "Original site https://corybooker.com/ has 33 links within it.\n",
      "Original site https://joebiden.com/ has 33 links within it.\n",
      "Original site https://www.julianforthefuture.com/ has 26 links within it.\n"
     ]
    }
   ],
   "source": [
    "results = cur.execute('''SELECT base_url, count(*) as cnt FROM site_text GROUP BY base_url ORDER BY cnt DESC''')\n",
    "\n",
    "for row in results :\n",
    "    print(\"Original site {} has {} links within it.\".format(row[0],row[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we've identified all the pages we're going to be potentially pulling, let's go through our DB and figure out which ones we want to actually get the text for. \n",
    "\n",
    "I'll set a flag for the maximum number of pages we'll pull. We'll use a trick where for doing this. If the flag is negative then we'll do every link. Otherwise we'll set it to the max, unless the candidate has too few pages for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our function for pulling visible text\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed pull for https://www.donaldjtrump.com/.\n"
     ]
    }
   ],
   "source": [
    "max_pages = -1\n",
    "\n",
    "for candi_site in sites :\n",
    "    \n",
    "    # First, let's get a list of all the links. \n",
    "    links = []\n",
    "    \n",
    "    sql_query = \"SELECT url FROM site_text WHERE base_url = '{}' AND pulled = 0\".format(candi_site)    \n",
    "    link_results = cur.execute(sql_query)\n",
    "\n",
    "    for row in link_results :\n",
    "        links.append(row[0])\n",
    "            \n",
    "    # Now we take a sample of the links if max_pages is non-negative (and the candidate\n",
    "    # has enough links).\n",
    "    if max_pages < 1 or len(links) < max_pages :\n",
    "        pages_to_pull = links\n",
    "    else :\n",
    "        pages_to_pull = random.sample(links,max_pages)\n",
    "        \n",
    "    for page in pages_to_pull :\n",
    "        page_text = '' # initialize it to an empty string\n",
    "        \n",
    "        # Many of the pages are relative links. For instance, \n",
    "        # elizabethwarren.com has a link \"/plans\". We'd like \n",
    "        # to get that page, so we'll glue them together if \n",
    "        # the page doesn't have \"http\" in it.\n",
    "        \n",
    "        if page and not \"http\" in page : # test to make sure page isn't None\n",
    "            \n",
    "            if page[0] == r\"/\" : # trump links came through \"/about/\"\n",
    "                page = page[1:]\n",
    "\n",
    "            page = \"\".join([candi_site,page])\n",
    "        \n",
    "        try :\n",
    "            r = requests.get(page)\n",
    "        except :\n",
    "            pass \n",
    "\n",
    "        if r.status_code == 200 :\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            texts = soup.findAll(text=True)\n",
    "            visible_texts = filter(tag_visible, texts) \n",
    "            page_text = \" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "        # Now let's update our DB row.\n",
    "        # We should be able to do this in a single UPDATE statement, \n",
    "        # but I had to do it with one for each field we're updating. \n",
    "        sql_query = ('UPDATE site_text '\n",
    "                     'SET text = ? '\n",
    "                     'WHERE url = ? ' )\n",
    "        \n",
    "        # Two items in a list matches up with the two \"?\"\n",
    "        cur.execute(sql_query,[page_text,page])\n",
    "        \n",
    "        sql_query = ('UPDATE site_text '\n",
    "                     'SET pulled = 1 '\n",
    "                     'WHERE url = ? ' )\n",
    "        \n",
    "        # Two items in a list matches up with the two \"?\"\n",
    "        cur.execute(sql_query,[page])\n",
    "\n",
    "        sql_query = ('UPDATE site_text '\n",
    "                     'SET dt = ? '\n",
    "                     'WHERE url = ? ' )\n",
    "        \n",
    "        # Two items in a list matches up with the two \"?\"\n",
    "        cur.execute(sql_query,[datetime.datetime.now(),page])\n",
    "        \n",
    "    print(\"Completed pull for {}.\".format(candi_site))\n",
    "\n",
    "db.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
