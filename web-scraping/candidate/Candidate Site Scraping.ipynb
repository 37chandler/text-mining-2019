{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Candidates' Sites\n",
    "\n",
    "In this notebook we'll scrape candidates' websites and store the information for later analysis. We'll store the data locally in a database, which we'll build via Python to practice that skill. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4.element import Comment\n",
    "\n",
    "import random \n",
    "# this package is good for randomization, which we may use \n",
    "# use when we're pulling a fraction of the pages. \n",
    "\n",
    "import datetime # this is good for working with dates and times. It's a bit confusing though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build a DB for this. Right now I'm assuming we'll have five fields in our only table:\n",
    "\n",
    "* `dt`: the date and time when we pulled the page.\n",
    "* `base_url`: the main URL we're pulling from. E.g., www.joebiden.com, for Biden's site. \n",
    "* `url`: the specific URL we're pulling from. E.g., https://joebiden.com/joes-story/.\n",
    "* `text`: the text of the `url`. \n",
    "* `pulled`: A boolean that is TRUE if we've tried to pull the text from the URL. This is useful for keeping track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\"candidate_websites_2.db\") # feel free to change this to something you like. \n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the table in the DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS site_text''')\n",
    "cur.execute('''CREATE TABLE site_text (\n",
    "    dt DATETIME, \n",
    "    base_url TEXT, \n",
    "    url TEXT,\n",
    "    text TEXT,\n",
    "    pulled BOOLEAN)''')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build off the previous notebook (`Intro to Scraping.ipynb`) to scrape candidates' sites. Let's begin by reading the list of websites.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = []\n",
    "with open(\"candidates_websites.txt\",'r') as infile :\n",
    "    for line in infile :\n",
    "        sites.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn\n",
    "\n",
    "Here's where you take over. We want to pull all the text from each candidate's webpage. This is probably a two step process. Step one is to get all the links from the homepage. Step two is to follow each link, grab the text, and store it in the DB. Try starting small, by just pulling a single page and putting it into the DB. Here's an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_url = \"https://facts.elizabethwarren.com/\"\n",
    "r = requests.get(this_url)\n",
    "\n",
    "# get the visible text from this page. Use the Intro notebook as your guide!\n",
    "page_text = \"some text that you'll replace with the result of excellent work!\"\n",
    "\n",
    "new_row = [datetime.datetime.now(),\n",
    "           \"https://elizabethwarren.com\",\n",
    "           this_url,\n",
    "           page_text, \n",
    "           1]\n",
    "\n",
    "cur.execute('''INSERT INTO site_text (dt,base_url,url,text,pulled) \n",
    "       VALUES (?,?,?,?,?)''',new_row)\n",
    "\n",
    "db.commit() # don't forget this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this got you one page. Now see if you can get a list of every link on *one* candidate's homepage and put that text into the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
