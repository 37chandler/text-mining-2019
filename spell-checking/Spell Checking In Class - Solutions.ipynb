{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell checking\n",
    "\n",
    "This workbook holds some exploration of the spell checking code.\n",
    "\n",
    "Our model for spell checking requires four main steps: \n",
    "\n",
    "1. Selecting which known word is the highest priority. \n",
    "1. Coming up with the candidates that _could_ be the correct spelling of an unknown word. \n",
    "1. The language model: the probability of a given candidate.\n",
    "1. The error model: Given that $c$ is correct, how probable is $P(w|c)$? \n",
    "\n",
    "There are some implied other tasks we'll need to take care of:\n",
    "\n",
    "* Discovering if a word is known.\n",
    "* Estimating the frequency of a word in a corpus (this is the language model). \n",
    "* Finding potential \"single edit\" candidates. \n",
    "* Finding potential \"double edit\" candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding known words\n",
    "\n",
    "Let's start by exploring the notion of \"known words\". The file `wsj_with_errors.txt` has a bunch of text in it with errors introduced. \n",
    "\n",
    "1. Ingest this file one word at a time.\n",
    "1. Remove punctuation and numbers.\n",
    "1. Cast to lowercase.\n",
    "1. Take every word thatâ€™s not in the NLTK word corpus and write it to a file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_words = {w.lower() for w in nltk.corpus.words.words()}\n",
    "word_regex = re.compile(r'^[a-z]+$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here. \n",
    "unknown = set()\n",
    "with open(\"wsj_with_errors.txt\",'r') as infile :\n",
    "    for line in infile :\n",
    "        line = [w.lower() for w in line.strip().split()]\n",
    "        line = [w for w in line if word_regex.search(w)]\n",
    "\n",
    "        for word in set(line) - nltk_words :\n",
    "            unknown.add(word)\n",
    "\n",
    "                \n",
    "with open(\"in_wsj_not_nlkt.txt\",'w') as outfile :\n",
    "    for word in unknown :\n",
    "        outfile.write(word + \"\\n\")\n",
    "\n",
    "print(len(unknown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file `big.txt` has 1.1M words in it. Repeat the above steps but use this as our set of known words. Compare the results. How much of a difference does the big corpus provide? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_big_words = list() # useful below\n",
    "\n",
    "with open(\"big.txt\") as infile :\n",
    "    for line in infile :\n",
    "        line = [w.lower() for w in line.strip().split()]\n",
    "        line = [w for w in line if word_regex.search(w)]\n",
    "        \n",
    "        for word in line :\n",
    "            all_big_words.append(word)\n",
    "\n",
    "            \n",
    "big_words = set(all_big_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more of your code here. \n",
    "unknown = set()\n",
    "with open(\"wsj_with_errors.txt\",'r') as infile :\n",
    "    for line in infile :\n",
    "        line = [w.lower() for w in line.strip().split()]\n",
    "        line = [w for w in line if word_regex.search(w)]\n",
    "\n",
    "        for word in set(line) - big_words :\n",
    "            unknown.add(word)\n",
    "\n",
    "with open(\"in_wsj_not_big.txt\",'w') as outfile :\n",
    "    for word in unknown :\n",
    "        outfile.write(word + \"\\n\")\n",
    "\n",
    "print(len(unknown))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Word Frequency\n",
    "\n",
    "Take the `big.txt` file and create a variable called `WORDS` that is a frequency distribution of the words in `big.txt`. Again, remove punctuation and numbers and cast to lowercase. (`WORDS` should be a `Counter` object.) Answer some questions with it:\n",
    "\n",
    "* How many times does the word \"the\" appear? \n",
    "* What fraction of the words are \"the\"?\n",
    "* What is the 30th most common word?\n",
    "* How many words appear a single time? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "WORDS = Counter(all_big_words)\n",
    "\n",
    "print(WORDS['the']) # 78172\n",
    "print(WORDS['the']/sum(WORDS.values())) # 0.0853\n",
    "print(WORDS.most_common(30)) # they is #30\n",
    "print(len([w for w, c in WORDS.items() if c == 1])) # 7994\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do this efficiently, we're going to need some functions. Write a function called `known` that takes a list of words and returns a _set_ of the words in that list which are known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "    \n",
    "    \n",
    "    \n",
    "assert(known(['hello','help','halp'])=={'hello','help'})\n",
    "assert(known(['a','bunch','of','words'])=={'a','bunch','of','words'})\n",
    "assert(known(['0ops','l33t','qwjibo'])==set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a function called `P` that takes a word and returns that word's fraction in the corpus. So `P(\"the\")` should return `0.08533967969781989`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word) :\n",
    "    N = sum(WORDS.values())\n",
    "    return (WORDS[word]/N)\n",
    "\n",
    "\n",
    "assert(P(\"the\")==0.08533967969781989)\n",
    "assert(P(\"happy\")==0.00018012903789259941)\n",
    "assert(P(\"fall\")==0.00011571926070676085)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Finding Single Edit Mistakes\n",
    "\n",
    "Our error model requires us to find single edit mistakes in a word. As we covered in the lecture, there are four types of single-edit mistakes: deletions, transpositions, replacements, and insertions. \n",
    "\n",
    "To start, we follow a clever approach from Norvig. What does this next cell do?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"monkey\"\n",
    "splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this hint and try to make all *insertions* first. That's when we just include a random letter somewhere in the word. For the word `monkey`, how many insertions are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "insertions = [L + c + R for L,R in splits for c in letters]\n",
    "print(len(insertions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up next, let's try to make the replacements. Now we're replacing a letter with a random letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make deletions and transpositions. These are a bit trickier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now write a function that takes a word as input and returns a _set_ all of the words that are one edit away. Call this `edits1`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word) :\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    \n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "\n",
    "assert(len(edits1('a'))==78)\n",
    "assert(len(edits1('apple'))==284)\n",
    "assert(len(edits1('applegate'))==492)\n",
    "assert(len(edits1('ambidexterous'))==702)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to get a lot of possible edits. How many of the above are known? Run them through your previous function. Also try out some other words to make sure your functions are working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known(edits1(\"monkey\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known(edits1(\"apple\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "known(edits1(\"text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Spell Checking\n",
    "\n",
    "We're close! The mechanics of our algorithm are as follows: \n",
    "\n",
    "1. Start with a word.\n",
    "1. If it's known, return that word. \n",
    "1. If it's not known, calculate the known words one edit distance away. \n",
    "1. If there are known words at one edit distance, return the highest probability one.\n",
    "1. Else, calculate the known words at two edit distances away. \n",
    "1. If there are any, return the one with the highest probability. \n",
    "1. Otherwise, return the word itself. \n",
    "\n",
    "In the cell below, create a function called `correction` that carries out these steps. It should take a word as the argument and will make use of your `WORDS` variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correction(word) :\n",
    "\n",
    "    if known([word]) :\n",
    "        return(word)\n",
    "    else :\n",
    "        ed_1 = edits1(word)\n",
    "        candidates = known(ed_1)\n",
    "        if candidates :\n",
    "            return(max(candidates, key=P))\n",
    "        else : \n",
    "            ed_2 = (e2 for e1 in ed_1 for e2 in edits1(e1))\n",
    "            candidates = known(ed_2)\n",
    "            if candidates :\n",
    "                return(max(candidates, key=P))\n",
    "            else :\n",
    "                return(word)\n",
    "            \n",
    "\n",
    "assert(correction(\"thew\")==\"the\")\n",
    "assert(correction(\"tst\")==\"test\")\n",
    "assert(correction(\"mano\")==\"man\")\n",
    "assert(correction(\"myxomatosis\")==\"myxomatous\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
