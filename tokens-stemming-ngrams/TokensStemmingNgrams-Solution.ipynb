{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install the `nltk` book corpus. If you haven't done it before, here are the steps:\n",
    "\n",
    "1. Open a console or command window.\n",
    "1. Type `python` to start using python. \n",
    "1. Type `import nltk` and hit enter.\n",
    "1. Type `nltk.download()` and hit enter.\n",
    "1. This will open a little window. \n",
    "1. Click \"All Packages\" at the top of the list. \n",
    "1. Click \"Download\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Questions:\n",
    "\n",
    "1. Find emojis in the chat corpus.\n",
    "\n",
    "1. Determine a normalization scheme. (What needs to be normalized, how would you do it?)\n",
    "\n",
    "1. Count the happy vs sad emojis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = text5 # give it a nice name. \n",
    "\n",
    "# Let's find emojis in chat. \n",
    "potential_emojis = {w for w in chat if \":\" in w or \";\" in w or \"=\" in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly we're catching some non-emojis, but let's assume we're getting most of the list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count happy vs sad\n",
    "happy = [w for w in chat if w in {\":-)\",\":)\",\":D\",\";-)\",\"=)\"}]\n",
    "sad = [w for w in chat if w in {\":-(\",\":(\",\";-(\",\"=(\"}]\n",
    "\n",
    "print(len(happy))\n",
    "print(len(sad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some normalization of a plain text corpus. The repo includes a file \"beowulf.txt\". Read this in, split it into words, and let's count the number of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beowulf = [] # a list per line is one option\n",
    "\n",
    "with open(\"beowulf.txt\",'r') as infile :\n",
    "    for line in infile :\n",
    "        beowulf.append(line.strip())\n",
    "        \n",
    "# Lots of blanks we don't need. \n",
    "beowulf = [line for line in beowulf if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_words = []\n",
    "for line in beowulf :\n",
    "    beo_words.extend(line.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_counter = Counter(beo_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Let's go through some stemming examples from the NLTK. First, let's continue to practice exploring words. How many words in our Beowulf text end in \"ing\"? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beo_ing = []\n",
    "for word, count in beo_counter.items() :\n",
    "    if word[-3:] == \"ing\" :\n",
    "        beo_ing.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(beo_ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about in the NLTK words corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's something to get you started:\n",
    "for idx, word in enumerate(nltk.corpus.words.words()) :\n",
    "    print(word)\n",
    "    if idx > 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ing = []\n",
    "\n",
    "for word in nltk.corpus.words.words() :\n",
    "    if word[-3:] == \"ing\" :\n",
    "        all_ing.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_ing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the Beowulf words that aren't in the words corpus? Are there some in there that you think should be? Could you modify the Beowulf processing to have a higher hit rate? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in beo_ing if w not in all_ing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This finds more:\n",
    "[w for w in beo_ing if w.lower() not in all_ing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to stemming. Let's use the Porter Stemmer to look at some inaugural speech texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer() # give it a short name.\n",
    "start = 30000\n",
    "distance = 100\n",
    "\n",
    "print(\" \".join(text4[start:(start + distance)]))\n",
    "print(\"\\n\\n\")\n",
    "print(\" \".join([porter.stem(w) for w in text4[start:(start + distance)]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words are in the inaugural addresses? How many stems are in them? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words in inaugural addresses\n",
    "print(len(set(text4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inaug_stemmed = {porter.stem(w.lower()) for w in text4}\n",
    "\n",
    "print(len(inaug_stemmed))\n",
    "\n",
    "print(len(set(text4))/len(inaug_stemmed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Language Models\n",
    "Let's find some common n-grams in _Sense and Sensibility_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = FreqDist(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.freq('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the `isalpha` function helps us identify strings of ASCII letters \n",
    "print(\"abc\".isalpha())\n",
    "print(\"abc123\".isalpha())\n",
    "print(\"_\".isalpha())\n",
    "print(\"Hi!\".isalpha())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now go through the book (text2) and build a new frequency distribution. Build one with all of the following attributes:\n",
    "\n",
    "1. Lowercase words\n",
    "1. Words that _aren't_ in the `stopword` list\n",
    "1. Words that pass the `isalpha` test. \n",
    "\n",
    "What's the count of \"the\" in both frequency distributions? How have the most common words changed? (Use the `most_common` method on the frequency distribution.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd2 = FreqDist([w.lower() for w in text2 \n",
    "                if w.lower() not in \n",
    "                nltk.corpus.stopwords.words(\"english\") \n",
    "                and w.isalpha()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fd[\"the\"])\n",
    "print(fd2[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd2.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum up the total words in this second frequency distribution. Display the 20 most common words, their count, and their overall fraction of the books words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_words = sum([count for word, count in fd2.items()])\n",
    "\n",
    "for pairs in fd2.most_common(20) :\n",
    "    print(\" : \".join([pairs[0],str(pairs[1]),str(pairs[1]/total_words)]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this `FreqDist` function to look at common co-occurences of bigrams. NLTK provides a useful function, `ngrams`, that gives us the N-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_bigrams = nltk.ngrams(text2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, pair in enumerate(ss_bigrams) :\n",
    "    print(pair)\n",
    "    if idx > 20 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, `ngrams` returns an iterator, so we have to re-initialize it to use it.\n",
    "ss_bigrams = nltk.ngrams(text2,2)\n",
    "# Ask me about this if it doesn't make sense. And sensibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a frequency distribution of the bigrams in S&S and look at the most common ones.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_bi_fd = FreqDist(nltk.ngrams(text2,2))\n",
    "ss_bi_fd.most_common(20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a new frequency distribution of bigrams where the following hold true:\n",
    "1. All words to lowercase\n",
    "1. No bigrams where *both* words are in the `stopword` list\n",
    "1. Words that pass the `isalpha` test. \n",
    "\n",
    "Build this and display the 20 most common words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_ss_bigrams = []\n",
    "\n",
    "for pair in nltk.ngrams(text2,2) :\n",
    "    first, second = pair\n",
    "    first = first.lower()\n",
    "    second = second.lower()\n",
    "    \n",
    "    if first not in sw or second not in sw :\n",
    "        if first.isalpha() and second.isalpha() :\n",
    "            clean_ss_bigrams.append((first,second))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bi_fd = FreqDist(clean_ss_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bi_fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concordance is a cool way to look at a word in context. Explore some of your more common words in bigrams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2.concordance(\"sister\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## N-gram models\n",
    "\n",
    "Let's make a function that takes in text, builds a freq dist and generates text with various n-grams. To do this, we'll need a function that gives us words from a frequency distribution probabilistically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def weighted_choice(freq_dist):\n",
    "    weight_total = sum([count for token,count in freq_dist.items()])\n",
    "    n = random.uniform(0, weight_total)\n",
    "    for token, count in freq_dist.items() :\n",
    "        if n < count:\n",
    "            return(token)\n",
    "        n = n - count\n",
    "    return(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kind of complicated, but it does what we expect. Play around with the following cell to see words from various texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_choice(FreqDist(text4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function that generates text of a given length, using the probabilistic approach to glue one word to another. Have it start with a text and the desired length of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unigram(text,length=10) :\n",
    "    fd = FreqDist(text)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(length) :\n",
    "        results.append(weighted_choice(fd))\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now play around with the various texts, generating nonsense sentences from them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_unigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_unigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_unigram(text5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge exercise: Do the same thing, but have it work with bigrams. This is harder, since you have a \"current word\" you want to glue text onto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_choice_ngram(cur_word,freq_dist) :\n",
    "    ''' Starts with a current word and randomly chooses \n",
    "        a following word based on the bigrams. '''\n",
    "    \n",
    "    # First, build list of tuples of the form\n",
    "    # ('a_word',count)\n",
    "    # where our freq_dist has an entry like \n",
    "    # ('cur_word','a_word',count)\n",
    "    sub_dist = {}\n",
    "    \n",
    "    for bigram, count in freq_dist.items() :\n",
    "        if bigram[0] == cur_word :\n",
    "            sub_dist[bigram[1]] = count\n",
    "    \n",
    "    return(weighted_choice(sub_dist))\n",
    "\n",
    "def generate_bigram(text,length=10,start=None) :\n",
    "    \n",
    "    if not start :\n",
    "        uni_fd = FreqDist(text)\n",
    "        start = weighted_choice(uni_fd)\n",
    "        \n",
    "    fd = FreqDist(nltk.bigrams(text))\n",
    "    \n",
    "    results = []\n",
    "    this_word = start\n",
    "    for i in range(length) :\n",
    "        this_word = weighted_choice_ngram(this_word,fd)\n",
    "        results.append(this_word)\n",
    "        \n",
    "    return(\" \".join(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bigram(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bigram(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_bigram(text5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
